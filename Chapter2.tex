\ifdefined\included
\else
\documentclass[english,a4paper,11pt,twoside]{StyleThese}
\include{formatAndDefs}
\sloppy
\begin{document}
\setcounter{chapter}{1} %% Numéro du chapitre précédent ;)
\dominitoc
\faketableofcontents
\fi

\chapter{Supervision for Human-Robot Interaction}
\minitoc

\label{ch:Sup}

\section{Role of the supervisor in the global architecture}

\label{sec:globalArchi}

One of the goals of my research group at LAAS-CNRS is to build a fully autonomous robot which interacts and performs Joint Actions with humans. To do so, an architecture for human-robot interaction has been developed and is constantly improved by the group. This architecture is composed of several modules and a simplified scheme of it can be found in Fig.~\ref{fig:GlobalArchi}.

\begin{figure}[!h]
	\centering
    \includegraphics[width=0.7\textwidth]{figs/Chapter2/archiGlobal.png}
    \caption{The global architecture for human-robot interaction implemented at LAAS-CNRS.}
    \label{fig:GlobalArchi}
\end{figure}
\accom{à mon sens il faut revoir ce schéma de l’architecture, il manque une base de données par exemple, non ?}

\paragraph{Sensorimotor layer:}
The lower level of the architecture is composed of modules which allow to communicate and control sensors and actuators. Among others, this layer is composed of modules interpreting sensors data to detect humans and objects and a module allowing to execute given trajectories by calling the adequate actuators.

\paragraph{Situation Assessment:}
The situation assessment is done by a software called TOASTER \cite{milliezThesis}. One of the functionalities of TOASTER is to build and maintain a consistent world state based on data coming from the sensorimotor layer. Geometric computations are done on this world state to compute symbolic predicates concerning the environment (e.g. \textit{<object, isOn, support>}, \textit{<object, isIn, box>}) and agents abilities and behavior \accom{behaviors ?} (e.g. \textit{<object, isVisibleBy, human>}, \textit{<object, isReachableBy, robot>}, \textit{human, isLookingToward, object}). TOASTER is also in charge of perspective taking: the previous predicates are constantly estimated and maintained \accom{not only from the robot’s point of view but also} from the point of view of all humans \acc{concerned and perceived in the current context ? (ou quelque chose d’approchant pour dire qu’il y a qd meme un focus)}.

\paragraph{Geometric Planner:}
In order to perform actions and movements adapted to the human proximity, our architecture is equipped with a geometric task and motion planner called GTP \cite{waldhart2016novel}. GTP allows to compute trajectories as well as objects placements and grasps in order to refine actions \accom{je ne comprends pas l’utilisation de refine ici} such as Pick or Place while taking into account the human safety and comfort. \acccom{j’aurais plutot dit un truc du style : GTP allows to compute trajectories as well as objects placements and grasps. It does at a level that is human understandable and readable by giving access to high level tasks such as Pick or Place (an object (e.g. a cube), somewhere (e.g. on a table)). Computed trajectory takes into account the human safety and comfort. }

\paragraph{Symbolic Planner:}
For the robot to be able to synthesize Shared Plans, our architecture is equipped with HATP, a human-aware HTN (Hierarchical Task Network) task planner which allows the robot to compute and refine a plan both for itself and its humans partners, taking into account a number of social rules \cite{Lallement2014hatp}.
\acccom{ce serait bien qu’apparaisse quelque part à quoi correspond le sigle HATP non ?}
HATP has been specially designed to integrate a number of features that are meant to promote the synthesis of plans that are acceptable by humans and easily if not trivially understandable by them. It allows to specify the humans and robot capabilities in terms of actions they can execute. Several aspects such as human preferences and comfort, estimation of human effort to achieve a task in a given context and "social rules" are used in a cost-based approach to build "sufficiently good" human-robot Shared Plans.

\paragraph{Dialogue Manager:}
In order for the robot to communicate with humans, a basic dialogue manager has been integrated to the architecture. This module allows to give humans information, ask basic questions and understand basic answers. \acccom{c’est surement voulu mais c’est trop vague, peut être y revient-on plus tard ?}

\paragraph{Supervisor:}
The last module of the architecture is the supervisor. It is the one in charge of controlling collaborative activities. It chooses the robot goals and monitors the Shared Plan execution. To do so, it estimates humans mental states concerning the Shared Plan and takes them into account to decide when to perform actions or to communicate (verbally and/or non-verbally). It also interprets the information coming from the Situation Assessment module \acccom{regarding the context ?} in order to recognize human actions like Pick or Place with regard to the Shared Plan. This module is an extension of \cite{clodic2009shary} and \cite{fiore2016planning} and is the major technical contribution of this thesis. Its internal architecture will be detailed in the next section.

\section{The supervisor architecture}

The supervisor is composed of several modules and is fully implemented in ROS\footnote{http://www.ros.org/}. The complete scheme of its architecture can be found in Fig.~\ref{fig:archiSup}, however, as the figure is quite complex, each composing part of the supervisor will be represented and described individually in the next subsections. 

\begin{figure}[!h]
	\centering
    \includegraphics[width=\textwidth]{figs/Chapter2/ArchiSup.png}
    \caption{Architecture of the supervisor.}
    \label{fig:archiSup}
\end{figure}

\subsection{Goal Manager}

The goal manager allows the robot to select and prioritize goals. It maintains a priority list of goals to perform which is updated with insert, abort or halt commands from dialogue or command line. 
\acccom{It maintains a priority list of goals to perform. This list is updated with insert, abort or halt commands from dialogue or command line.}
\acccom{est ce que tu expliques quelque part comment tu gères les priorités ? }

\begin{figure}[!h]
	\centering
    \includegraphics[width=0.5\textwidth]{figs/Chapter2/GoalManager.png}
    \caption{Interaction of the Goal Manager with the rest of the supervisor.}
    \label{fig:goalManager}
\end{figure}
\acccom{sur la figure, il y a marqué dialoge, d’après ce que tu as écrit avant il manque le u}

The chosen goal is published in order for the Plan Elaboration module to find a Shared Plan to perform it. The stop and suspend orders are transmitted \acccom{ce début de phrase n’est pas clair, je ne comprends pas} and the module receives reports from the Plan Elaboration module concerning the success of the goal \acccom{succès du but ou du calcul du plan ? et il n’y a pas que succès non ? (failed ?)}.

This module is, for now, really basic. An interesting extension would be to integrate data coming from an intention recognition module concerning humans activities. This will allow the robot to choose if it should proactively offer its help based on this data and the goal orders it received. 

\acccom{est ce qu’il n’y a qu’un et un seul but publié à chaque instant ? le lien entre la publication et les commandes « insert, abort et halt » ?}

\subsection{Plan elaboration}

\begin{figure}[!h]
	\centering
    \includegraphics[width=0.6\textwidth]{figs/Chapter2/PlanElaboration.png}
    \caption{Interaction of the Plan Elaboration with the rest of the supervisor.}
    \label{fig:planElaboration}
\end{figure}

Once a goal received from the Goal Manager, the Plan Elaboration module is in charge of finding a Shared Plan to perform it. To do so, the module is able to call HATP (the Human Aware Task Planner described in Sec.~\ref{sec:globalArchi}) to compute a plan and the dialogue module to validate the plan or ask for missing information \acccom{comment tu t’assures que çà ne se marche pas sur les pieds par exemple qd tu fais appel au dialogue vu qu’il y a plusieurs modules qui peuvent y avoir accès ? y a t’il un ordre de priorité ou un séquencement ?}. One of the contributions of this thesis concerns the elaboration of more flexible Shared Plans where some decisions are left to the execution. This work is in part done in this module and will be developed in Chapter~\ref{ch:SP}.

The computed Shared Plan is then published in order for the Plan Maintainer module to deal with it. The stop and suspend orders are transmitted \acccom{ce début de phrase n’est pas clair, je ne comprends pas} and the module receives reports from the Plan Maintainer module concerning the success, failure or need of adaptation of the Shared Plan.

\subsection{Plan Maintainer}

The Plan Maintainer module is in charge of following \acccom{monitoring ?} the execution of the Shared Plan based on the current world state and current and past actions. It publishes the list of actions from the Shared Plan which need to be performed at a given time \acccom{moment ?} and the list of actions from the Shared Plan which need to be done later. It also checks the consistency of the plan \acccom{est ce que tu expliques çà plus tard ?, en quoi çà consiste ?} and reports to the Plan Elaboration module in case of failure or unexpected situations.

\begin{figure}[!h]
	\centering
    \includegraphics[width=0.8\textwidth]{figs/Chapter2/PlanMaintainer.png}
    \caption{Interaction of the Plan Maintainer with the rest of the supervisor.}
    \label{fig:planMaintainer}
\end{figure}

\subsection{Human Monitor}

The Human Monitor module allows to interpret the current world state which contains humans activity information in order to recognize basic humans actions like Pick or Place. This module is, for now, really basic as it is based mainly on distances between humans and objects. However, there is room for improvements by taking into account the context during action recognition or using probabilistic models. \acccom{est ce que tu prends en compte l’action qu’est sensé faire la personne ? (i.e. genre goal-to-action ?) si oui tu prends déjà un peu en compte le contexte non ?}


\begin{figure}[!h]
	\centering
    \includegraphics[width=0.4\textwidth]{figs/Chapter2/HumanMonitor.png}
    \caption{Interaction of the Human Monitor with the rest of the supervisor.}
    \label{fig:humanMonitor}
\end{figure}

\newpage
\subsection{Mental State Manager}

The Mental State Manager estimates the humans mental states concerning the current goal and Shared Plan. It bases its reasoning on all data published by the other supervisor modules and on the world states from all agents point of view given by TOASTER (see Sec.~\ref{sec:globalArchi}). This work is one of the thesis contribution and will be developed in Chapter \ref{ch:MS}. The composition of the estimated mental states will be given in Sec.~\ref{sec:data}. 

\begin{figure}[!h]
	\centering
    \includegraphics[width=0.5\textwidth]{figs/Chapter2/MSManager.png}
    \caption{Interaction of the Mental State Manager with the rest of the supervisor.}
    \label{fig:MSManager}
\end{figure}

\subsection{Robot Decision}

The Robot Decision module allows the robot to decide which action to execute and which information to give. Its decisions are based on the lists of current, planned and to execute actions as well as on the humans mental states. The way the robot uses these mental states to give pertinent information to humans is one of the thesis contribution and will be developed in Chapter \ref{ch:MS}. The decision of which action to execute has also been studied in this thesis and will be developed in Chapter \ref{ch:SP}.

\begin{figure}[h]
	\centering
    \includegraphics[width=0.7\textwidth]{figs/Chapter2/RobotDecision.png}
    \caption{Interaction of the Robot Decision module with the rest of the supervisor.}
    \label{fig:robotDecition}
\end{figure}

The Robot Decision module sends commands to the Action Executor from which it receives reports. It also communicates with the dialogue module and the Non-Verbal Behavior module to give the correct information.

\subsection{Action Executor}

The Action Executor is in charge of supervising robot actions. It receives actions to execute and stop or suspend orders from the robot decision and calls lower level modules to perform the given action in the best possible way. The action execution has been studied in this thesis and will be developed in Chapter \ref{ch:Acting}.

\begin{figure}[!h]
	\centering
    \includegraphics[width=0.5\textwidth]{figs/Chapter2/ActionExecutor.png}
    \caption{Interaction of the Action Executor with the rest of the supervisor.}
    \label{fig:actionExecutor}
\end{figure}

\subsection{Non-Verbal Behavior}

This module allows to control the non-verbal behavior of the robot. In the current supervisor version, only the robot head behavior is concerned, but other type of non-verbal behaviors can be envisioned. The principles behind this module will be developed in Chapter \ref{ch:Acting}.

\begin{figure}[!h]
	\centering
    \includegraphics[width=0.5\textwidth]{figs/Chapter2/NVBehavior.png}
    \caption{Interaction of the Non-Verbal Behavior with the rest of the supervisor.}
    \label{fig:NVBehavior}
\end{figure}

The robot head behavior is based on the current robot action, the humans activities and the actions to perform. The module communicates with the dialogue module in order to coordinate and calls lower modules to control the robot. 

\section{Data representation}

\label{sec:data}

As seen in the previous section, several types of data are produced and used by the supervisor to take decisions. We will see now how we represent this data and this formalization will be used in the next chapters of the thesis.

The current state of the world from the robot point of view $WS$ is composed of a set of predicates $p$:
$$p = <entity, attribute, value>$$
For example, the fact that an object is on a table will be represented as $<object, isOn, table>$.

A goal $g$ is represented as:
$$g = <Name_g, Actors_g, Params_g, Obj_g>$$
where $Name_g$ allows to identify the goal, $Actors_g$ are the agents involved in the goal achievement, $Params_g$ are entities  (agents or objects) used to define precisely the goal and $Obj_g$ is a set of predicates representing the objective of the goal.
For example, if the robot has for goal to clean the table of the kitchen in collaboration with Bob by removing all items on it, this goal will be represented as:
$<Clean, <Robot, Bob>, <Kitchen\_table>, <<NULL, isOn, Kitchen\_table>>>$.
Finally, at its end, each goal $g$ is stored and associated with a label noted $label_g$ which can be equal either to DONE or ABORTED.

Then, a Shared Plan $SP$ is represented as:
$$SP = <id_p, A_p, L_p>$$
where $id_p$ is used to identify the plan, $A_p$ are the actions composing the plan and $L_p$ the links representing the order the actions should be executed (causal links).

A link $l \in L_p$ can be describe as:
 $$l = \langle prev_l, \ next_l \rangle$$
where $prev_l$ is the id of the action which needs to be achieved before the action with the id $next_l$ is performed. 

The actions composing the plan $A_p$ can be decomposed as:
$$A_p = <A_{prev}, A_{cur}, A_{next}, A_{later}>$$
where $A_{prev}$ are the actions of the plan already executed, $A_{cur}$ the actions currently executed, $A_{next}$ the actions which can be performed according to causal links and actions preconditions and $A_{later}$ the actions to be executed in the future.
Each of the set of actions previously introduced can be decomposed as:
$$<A = A^R, A^H, A^X>$$
where $A^R$ are the actions assigned to the robot, $A^H$ the actions assigned to the human and $A^X$ the actions not yet assigned. Indeed, we will see in Chapter~\ref{ch:SP} that not all actions are assigned to an actor during plan elaboration.
Finally, each action $a$ in $A_{prev}$ is associated with a label noted $label_a$ which can be equal either to DONE, FAILED or ABORTED.

An action $a$ can be represented as: 
$$a = < id_{a}, \ Name_{a}, \ Ag_{a}, \ Params_{a}, \ Precs_{a}, \ Effects_{a} >$$
Where $id_{a}$ is the action identifier and $Name_{a}$ represents its name. $Actors_{a}$ are the actors of the actions and $Params_{a}$ a set of parameters (objects or agents) which allows to define precisely the action. $Precs_{a}$ and $Effects_{a}$ are sets of predicates representing respectively the action preconditions and effects.
For example the action for the robot to place an object on a support will be defined as:$<0, place, Robot, <object, support>, <<object, isHoldBy, Robot>>, <<object, isOn, support>>>$.

The information concerning the state of the task is regrouped in what will be called the Task State $TS$:
$$TS = <g_R, SP, WS>$$
with $g_R$ the current goal of the robot, $SP$ the current Shared Plan and $WS$ the current world state from the robot point of view.

The robot also have a representation of humans mental state. The representation of the mental state $MS(H)$ of a human H is represented as:
$$MS(H) = <g_H, g_R(H), SP(H), WS(H)>$$
where $g_H$ is the goal the robot estimates the human is engaged in, $g_R(H)$ is the goal the robot estimates the human thinks the robot is performing, and $SP(H)$ and $WS(H)$ are the estimation of the Shared Plan and the World State from the human point of view.
$SP(H)$ is represented in the same way as the robot Shared Plan.


\ifdefined\included
\else
\bibliographystyle{StyleThese}
\bibliography{These}
\end{document}
\fi
