\ifdefined\included
\else
\documentclass[english,a4paper,11pt,twoside]{StyleThese}
\include{formatAndDefs}
\sloppy
\begin{document}
\setcounter{chapter}{3} %% Numéro du chapitre précédent ;)
\dominitoc
\faketableofcontents
\fi

\chapter{When to take decisions during Shared Plans elaboration and execution}
\minitoc

\label{ch:SP}

\section{Motivation}

When performing a Joint Action and more particularly when executing Shared Plans, several choices have to be made. Some of them are implicit, while others require a negotiation or an adaptation between the Joint Action participants. To be a good partner when performing Joint Action with humans, the robot should be able to identify which decisions are implicit and correctly communicate about the other ones. Indeed, a robot which communicate about each detail of a Shared Plan would easily become too "chatty" when a robot which do not communicate can be confusing.

Let's takes for example a robot helping a human to build a flat-pack table: the legs of the table need to be assembled with a hammer, the tray with a screwdriver and finally someone needs to put the tray on the legs. The robot is equipped with several tools including a screwdriver but no hammer and the human has only one hammer. The human and the robot are both able to put the tray on the legs. It is common sense that the robot should assemble the tray while the human assemble the legs. However, a decision needs to be taken concerning who will put the tray on the legs. 
In this scenario, the robot should assemble the tray without asking the human and negotiate or adapt its behavior to put the tray. 

The work presented in this chapter consists in finding which decisions are implicit or not, when the decisions should be taken and how to take them. We identify three types of decisions to be taken during Shared Plans elaboration and execution:
\begin{itemize}
\item \textbf{Which action perform in which order:} this is one of the biggest concern during Shared Plan elaboration. We do not focus our work in this part. Indeed, we use for this part HATP, a human-aware HTN planner which has been demonstrated to be well suited to human-robot joint action \cite{Lallement2014hatp}.
\item \textbf{Who will perform which action:} sometime this decision can be implicit when only one agent is able to perform an action. However, in others situations, the robot should be able to decide who will perform an action by negotiating or adapting its behavior to the human one.
\item \textbf{With which object:} for practical reason, the robot reason on objects by attributing them a unique id. However, for the purpose of an action, two objects can be semantically identical. When there is a choice in which object to use for an action, the robot should be able to adapt to the human behavior in order to avoid potential conflicts.
\end{itemize}

\section{Background}

When the robot needs to achieve a joint goal, several works allow it to compute plans which take into account the human (\cite{cirillo2010human,Lallement2014hatp}). They allow the robot to reduce resource conflicts \cite{chakraborti2016planning}, take divergent beliefs into account (\cite{guitton2012belief,talamadupula2014coordination}) or promote stigmergic collaboration
for agents in co-habitation \cite{chakraborti2015planning}. 

The relevance of using a Shared Plan in human-robot interaction has been studied by \cite{lallee2013cooperative}. They suggest that the joint plan should be fully communicated in order to sustain effective collaboration. Moreover, in \cite{gombolay2015decision}, it is shown that subjects prefer letting the robot plan when the task is too complex, prioritizing efficiency. 
In more simple tasks, a robot proactively helping the human is preferred to one waiting before proposing help \cite{baraglia2016initiative}. 

If the robot decides to share the plan, several studies have been reported on how to do communicate about the plan. Some researchers studied how a system could acquire knowledge on plan decomposition from a user \cite{Mohseni2015} and how dialog can be used to teach new collaborative plans to the robot and to modify these plans \cite{petit2013coordinating}. In \cite{sorce2015proof}, the system is able to learn a plan from a user and transmit it to another user. \cite{allen2002human} presents a computer agent able to construct a plan in collaboration with a user. Finally, in \cite{milliez2016using}, Milliez et al. present a system where the robot shares the plan with a level of details which depends on the expertise of the user. In our work, we try to get rid of the entire shared plan verbalization by taking the right decision at the right time in order to come up with a robot which communicates only when necessary.

Several contributions have been done to allow more adaptability during shared plan execution. \cite{chien2000using} proposes a method to plan only a few steps in advance and then plan the actions further in an iterative way. Chaski, a task-level
executive, presented in \cite{shah2011improved}, allows to choose when to execute the robot actions adapting to a human partner. A system which mixes plan recognition and adaptation is described in \cite{levine2014concurrent}. It computes all possibilities for the plan and chooses an action based on the choice of the human and causal links. \cite{hoffman2007effects} proposes an adaptive action selection mechanism for a robotic teammate, making anticipatory decisions based on the confidence of their validity and their relative risk. \cite{karpas2015robust} presents Pike, an online executive that unifies intent recognition and plan adaptation for temporally flexible plans with choice. Finally, this work is based on SHARY \cite{clodic2009shary} which was extended in \cite{fiore2014planning}, a supervisor allowing to execute human-aware shared plans taking into account joint actions aspects like reactive action execution.

In the cooperative multi-robot literature, task allocation and cooperative activity achievement has been thoroughly investigated \cite{gerkey2004formal}. Auction has been used very successfully for distributed multi-robot in various contexts (\cite{gerkey2002sold,botelho1999m+}). 
Concerning the so-called teamwork and cooperative task achievement taking into account explicit constraints to facilitate the activity of the other robots or agents activity, one can mention \cite{tambe1997agent} and \cite{joyeux2009plan}. While these contributions have inspired work on human-robot collaboration, it is however important to exhibit some essential aspects in such a context. Indeed, the human and the robot are not equal in any aspect. The robot is here to help the human and facilitate his activity.

In AI, the goal reasoning domains deals with some problems similar to Shared Plan management \cite{molineaux2010goal, roberts2016goal}. The role of goal reasoning is to survey the current goals of a robot, check that they remain feasible and relevant and establish new goals if needed. Moreover, part of the goal reasoning function is sometimes linked to the plan management as it is in charge of deciding when and how to generate plans (but it is not producing the plan) and checking for unexpected events. 

\section{Assumptions}

The work presented in this chapter treats of the needed decision during Shared Plan elaboration and execution. The focus is put here into the decisions concerning the action allocation and instantiations. To do so, we make several assumptions:

\paragraph{Single human:} the work presented in this chapter has been designed for a robot interacting with a single human. However, all the data structures and main principles are compatible with multi-humans set-up.

\paragraph{Commitment:} we do not focus in this works on issues related to commitment. Consequently, we consider here that the joint goal has already been established. We also consider that the human will not abort the goal unless he knows that the goal is not achievable any more.

\paragraph{Shared Plan:} we put the focus here on the issues related to action allocation and instantiation. In order to decide which action to execute in which order, we use HATP, a human-aware HTN planner which has been demonstrated to be well suited to human-robot Joint Action \cite{Lallement2014hatp}.
We are focusing in this work about medium complexity Shared Plans where the human might want to decide for his own actions.

\paragraph{Humans perception:} we make the assumption here that a human will see and understand an action of the robot when he is present and looking at it. We also assume that when he is present, the human is able to hear and understand the information verbalized by the robot.

\paragraph{Robot capacities:} we consider that the robot is able to perform simple high level actions like Pick, Place or Drop. We also assume that the robot is able to ask to the human if he wants to perform an action and to understand a basic answer (yes/no type). 
The robot is able to detect and localize objects and agents
and to recognize simple high level actions performed by the human like Pick, Place or Drop. Let us also note that the ways the robot achieves actions (e.g. human-aware motion planning and execution) and recognizes human's actions are outside of the scope of this chapter.

\paragraph{Communication:} the focus of this work is more on the what to communicate and when than on the how to communicate. Here we use the basic dialogue module described in Chapter~\ref{ch:Sup} to communicate with the human but more complex communication mechanisms can be envisioned. 

\section{Main principles}

We will present in this section the main principles we use for Shared Plans management. Three main algorithms are used to allow the robot to elaborate and execute Shared Plans. They exchange between then through the Shared Plan data \textit{SP} and several signals (noted \textit{S\_X} where \textit{X} is the name of the signal). These three algorithm run constantly and in parallel.

The Alg.~\ref{alg:mainPlan} allows the robot to elaborate a Shared Plan when needed, maintain the current Shared Plan and manage the human mental states.

\begin{algorithm}
\caption{Shared Plan management}
\label{alg:mainPlan}
\begin{algorithmic}
\WHILE {$g_R$}
\IF {$!SP \ \| \ S\_needReplan$}
\STATE $SP \leftarrow PLAN(g_R, WS)$
\ENDIF
\IF {S\_needUpdate}
\STATE $SP \leftarrow UPDATE\_PLAN(SP, WS)$
\ENDIF
\IF {S\_actionAllocated}
\STATE $SP \leftarrow EVALUATE\_ PLAN(SP, WS)$
\ENDIF
\IF {$Obj_g \in WS$ \hfill \textit{$\vartriangleright$ The goal is achieved}
\STATE}
\STATE $g_R \leftarrow \emptyset$ 
\STATE $SP \leftarrow \emptyset$
\ENDIF
\IF {$<Human, isPresent, true> \in WS \ \& \ (WS(H) \neq WS(H)_{t-1} \  \|  \  TS \neq TS_{t-1})$}
\STATE $MS(H) \leftarrow ESTIMATE\_MS(MS(H), TS)$
\IF {$MS(H) \neq TS$ \hfill \textit{$\vartriangleright$ Divergent belief}
\STATE}
\STATE $SOLVE\_DB(MS(H), TS)$
\ENDIF
\ENDIF
\ENDWHILE
\end{algorithmic}
\end{algorithm} 

When the robot has a goal $g_R$ to achieve and no current plan or when a signal is received to compute a new plan ($S\_needReplan$), the robot computes a Shared Plan to perform the goal based on the current world state $WS$ (see Sec.~\ref{subsec:elaboration}):
$$SP \leftarrow PLAN(g_R, WS)$$

When an action $a$ from the plan is performed by an agent a signal is received ($S\_needUpdate$) and the robot updates the plan (see Sec.~\ref{subsec:maintaining}):
$$SP \leftarrow UPDATE\_PLAN(SP, a)$$

When an action from $A^X$ has been allocated ($S\_actionAllocated$), the robot looks for the consequences of this allocation in the plan (see Sec.~\ref{subsec:allocation}):
$$SP \leftarrow EVALUATE\_ PLAN(SP, WS)$$
In addition to this, the robot also constantly checks if the goal is reached (the objectives of the goal are in the current World State).
Finally, each time a change occurs in $WS(H)$ or $TS$, the robot estimates the human mental states as described in the previous chapter (see Chapter~\ref{ch:MS}):
$$MS(H) \leftarrow ESTIMATE\_MS(MS(H), TS)$$,
If there is a conflict between the knowledge of the robot and the human mental state, the robot tries to solve it (see Chapter~\ref{ch:MS}): 
$$SOLVE\_DB(MS(H), TS)$$

Then, Alg.~\ref{alg:mainExec} allows the robot to decide when to act and which action to perform. 

\begin{algorithm}
\caption{Robot action decision}
\label{alg:mainExec}
\begin{algorithmic}
\WHILE {$SP$}
\STATE $A_{next} \leftarrow GET\_NEXT\_ACTIONS(SP, WS)$
\IF {$A_{next} = \emptyset$ \hfill \textit{$\vartriangleright$ No more feasible actions}
\STATE}
\STATE $S\_needReplan$
\ELSIF {$\{A^R_{next} \cup A^X_{next}\} =  \emptyset$ \hfill \textit{$\vartriangleright$ No actions for the robot
\STATE}}
\STATE $actionExecuted \leftarrow WAIT\_ACTION(A^H_{next}, t)$
\IF {!actionExecuted}
\STATE $S\_needReplan$
\ENDIF 
\ELSE
\STATE $a \leftarrow SELECT\_ACTION\_TODO(A_{next})$
\IF {$a \in A^X_{next}$}
\STATE $actor \leftarrow ALLOCATE\_ACTION(SP, a, WS, Prefs)$
\STATE $S\_actionAllocated$
\ENDIF
\IF {$a \in A^R_{next} \ \| \ (a \in A^X_{next} \ \& \ actor = robot)$}
\STATE $success \leftarrow EXECUTE(a)$
\IF {success}
\STATE $S\_needUpdate$
\ELSE 
\STATE $S\_needReplan$
\ENDIF
\ELSIF {$(a \in A^X_{next} \ \& \ actor = human)$}
\STATE $a \rightarrow A^H_{next}$
\ENDIF
\ENDIF
\ENDWHILE
\end{algorithmic}
\end{algorithm} 

When the robot has a Shared Plan $SP$, it looks for the action of this plan which need to and can be executed (see Sec. \ref{subsec:maintaining}):
$$A_{next} \leftarrow GET\_NEXT\_ACTIONS(SP, WS)$$
If there is no actions in $A_{next}$ nor actions in progress, it means that the plan is blocked, so the robot looks for another Shared Plan. If there is actions to do, the robot looks if there is an action it can execute (actions allocated to it or not allocated yet). If there is no, the robot waits the human performs an action ($A_{next}$ contains only actions from $A^H_{next}$):
$$actionExecuted \leftarrow WAIT\_ACTION(A^H_{next}, t)$$
If after a time \textit{t}, the human did not execute any action, the robot looks for another plan.
If there is actions the robot can execute, the robot selects an action $a$ (see Sec. \ref{subsec:selection}):
$$a \leftarrow SELECT\_ACTION\_TODO(A_{next})$$
If the selected action is not yet allocated, the robot first tries to allocate it (see Sec. \ref{subsec:allocation}):
$$actor \leftarrow ALLOCATE\_ACTION(SP, a, WS, Prefs)$$
If the action is allocated to the robot (after selection or allocation), the robot executes it:
$$success \leftarrow EXECUTE(a)$$
It will first instantiate the action if needed and then launch its execution (see Sec. \ref{subsec:execution}). If the action succeeds, the robot updates the plan, else it looks for another plan.

In parallel to the other execution loops, the robot is constantly monitoring human activities (Alg.~\ref{alg:monitoring}).

\begin{algorithm}
\caption{Human monitoring}
\label{alg:monitoring}
\begin{algorithmic}
\WHILE {$<Human, isPresent, true> \in WS$}
\IF {$\exists \ a \in A^H_{cur}$}
\IF {$a \in A^R_{cur}$}
\STATE $S\_stop$
\ENDIF
\STATE WAIT\_END\_ACTION(a)
\IF {$a \in A^H_{next}$}
\STATE $S\_needUpdate$
\ELSIF {$a \in A^X_{now}$}
\STATE $S\_actionAllocated$
\ELSE 
\STATE $S\_needReplan$ \hfill \textit{$\vartriangleright$ Unexpected action}
\ENDIF
\ENDIF
\ENDWHILE
\end{algorithmic}
\end{algorithm}

When the human performs an action, the robot first looks if the action is concurrent to the one it is performing (for example, if the human picks an object the robot was going to pick), and if it is the case, the robot stops its actions.  
Then, if the human performs an expected action with respect to the plan and already allocated to him, the robot updates the plan accordingly. If the action executed by the human is expected with respect to the plan but was not yet allocated, the robot looks for the consequences of this actions in the plan (see Sec. \ref{subsec:allocation}).
If the human performs an unexpected action with respect to the plan (action not in $A_{next}$ or in $A^R_{next}$), the robot looks for a new plan from the new situation induced by the human action.

The next sections will define more precisely the operators we just defined.


\section{Shared Plans elaboration}

The first step of this work is to be able to compute Shared Plans flexible enough to let part of the decisions to execution. This step correspond to the operator:
$$PLAN(g_R, WS)$$

As stated before, the human-aware HTN task planner HATP is used in this work to compute Shared Plans taking into account a number of social rules for both the robot and its human partner \cite{Lallement2014hatp}. However, in order to obtain flexible plans with HATP, a number of issues have to be considered.

First, when HATP returns a plan, it returns only one, which is assumed to be the best plan it has found given the situation and the associated costs. However, this plan is not always the only one possible (even at constant cost or computing time). Indeed, in such case, HATP makes some choices that could be preferably done on-line. For example, it can happen that one action can be done by several agents at the same cost. In a collaborative setting and more particularly when the human is concerned, it could be interesting to let the agents decide at execution time (or whenever it is interesting) who will do what. To handle this, we have adapted HATP by inserting what we call the\textit{X agent}. The capabilities of the \textit{X agent} correspond to the intersection of the capabilities of the human and the robot with a lower cost. Consequently, it will be chosen by the planner instead of the human or the robot whenever it is possible. If HATP returns a plan containing an action to be done by the \textit{X agent}, it means that this action could either be performed by the human or the robot. The decision concerning who will finally do this action is postponed. We will see in Sec. \ref{subsec:allocation} how \textit{X agent} actions will be finally allocated to the human or the robot.

As inputs to a planner such as HATP, we give a set of objects that are present in the environment and on which it will be able to apply its operators. Basically, each object is tagged and is unique. That means that if we have two times the same object, they will be uniquely tagged (e.g. two identical red cubes will be taged as RED\_CUBE\_1 and RED\_CUBE\_2). When two similar objects can be used in a same way during a task, the planner will choose either one or the other when it will to. In a collaborative setting, it could be counter intuitive since even if there is no distinction between the two objects at planning time, there can be one during execution. To handle this, we have adapted HATP by inserting the notion of \textit{similar} objects which aims to group interchangeable objects under a common name: two \textit{similar} objects will have the same role in the task. 

Finally, rather than in other works where we used HATP, we do not consider here that an agent is only capable to perform one action at a time. This allows the human to choose the order of his action when there is no impact in the global plan.

\section{Shared Plans execution}

We will now see in more detail how the robot executes the flexible Shared Plans obtained.

\subsection{Plan maintaining}
\label{subsec:maintaining}

First, the robot needs to be able to follow the Shared Plan execution and to determine which actions need to be executed and which actions need to be left for latter. 

As said before, the actions composing a Shared Plan can be decomposed as:
$$A_p = <A_{prev}, A_{cur}, A_{next}, A_{later}>$$

By default, when a Shared Plan is computed by the robot, all actions are put in $A_{later}$. When the robot performs an action or detects an action execution from a human, the executed action goes in $A_{cur}$ and, at the end of the execution, the action goes in $A_{prev}$ with a label equal either to DONE if the execution has been successful, FAILED if not or ABORTED if the robot had to stop the execution for an external reason.

An action will be put in $A_{next}$ if all previous actions in the plan are DONE (based on causal links) and its preconditions are checked:

$$a \in A_{next} \Leftrightarrow Precs_{a} \in WS \ \& \ (\forall l \in L_p \ | \ next_l = id_a,$$ 
$$\exists \ ap \in A_{prev} \ | \ (id_{ap} = prev_l \ \& \ label_{ap}  = DONE))$$

The $UPDATE\_PLAN$ operator updates the state of each actions of the plan and the $GET\_NEXT\_ACTIONS$ returns the actions in $A_{next}$.


\subsection{Action selection}
\label{subsec:selection}

Once the robot knows which actions need to be executed, it needs to choose one into the set of actions it can execute. To do so, it uses the $SELECT\_ACTION\_TODO$ operator which returns the action with the higher priority:

$$\underset{a \in \{A^R_{next} \cup A^X_{next}\}}{\mathrm{argmax}} \ priority(a)$$

\textbf{Priorities used:}
In our case, we have chosen to give higher priority to the actions allocated to the robot compared to those not already allocated. In general for this work, we have made the choice to postpone as much as possible the decisions made by the robot. Indeed, this choice is made in order to let to the human the maximum latitude, which allows him to take the initiative until the last possible moment. In the current implementation of our system, the priorities of the different actions of the robot are the same, so the robot will simply select one. However, there is a possibility to later integrate costs as, for example, select the action the farthest of what the human is currently doing. Concerning the priorities of not allocated actions, we still follow the principle to postpone at most the robot decision. To do so, we put a higher priority on what we call \textit{analogous} actions. Two actions will be \textit{analogous} when they have exactly the same decomposition (same action name and same parameters). 

\subsection{Action allocation}
\label{subsec:allocation}

Once a not yet allocated action is selected, the robot needs to decide if it should execute it or not ($ALLOCATE\_ACTION$ operator). To do so, the robot first looks for the possible actors of this action: agents which verify the preconditions of the action and which are not already busy. Note that even if the action was not allocated by HATP it is possible that there is only one possible actor. In this case, the robot automatically allocates the action to this agent. For example if the human is currently busy and there is an not allocated action  to perform, the robot will execute it. If there is more than one possible actor for the action, the robot follows the algorithm \ref{alg:allocate}.

\begin{algorithm}
\caption{Action allocation: $SP \leftarrow ALLOCATE\_ACTION(SP, a, WS, Prefs)$}
\label{alg:allocate}
\begin{algorithmic}
\REQUIRE $a \in A^X_{next}$
\IF {$cost(a, R) << cost(a, H)$}
\STATE $actor \leftarrow robot$
\ELSIF {$cost(a, H) << cost(a, R)$}
\STATE $actor \leftarrow human$
\ELSIF {node = negotiation}
\STATE $answer \leftarrow ASK(a, H)$
\IF {answer = yes}
\STATE $actor \leftarrow human$
\ELSE
\STATE $actor \leftarrow robot$
\ENDIF
\ELSE
\STATE $actionPerformed \leftarrow WAIT\_ACTION(a, t)$
\COMMENT adaptation mode
\IF {$actionPerformed$}
\STATE $actor \leftarrow human$
\ELSE
\STATE $actor \leftarrow robot$
\ENDIF
\ENDIF
\RETURN $actor$
\end{algorithmic}
\end{algorithm}

Then, the robot compares what it considers it costs to the human to perform the action and what it considers it costs to itself to perform the action. If it considers it is significantly less costly for it to perform the action, it will allocate the action to itself. Else, we have developed two possible modes for the robot. In the first mode, called \textbf{negotiation} mode, the robot directly asks to its human partner if he wants to perform the action and then allocates the action according to his answer. In the other mode, called \textbf{adaptation} mode, the robot waits a certain amount of time, and, if the human does not take the initiative to perform the action, it executes it. 

Allocating an action to an agent can lead to other actions being automatically allocated. For this reason, after each allocation of an action, a new plan is built taking into account the possible allocations of the actions remaining in $A_X$.

\textbf{Costs used for action selection:}
In the current implementation, we use a cost concerning the \textit{analogous} actions (see description in the previous subsection). These \textit{analogous} actions will have a lower cost for the robot to execute them leading the robot to automatically execute one of them. Indeed, the robot considers that since there are two or more same actions to execute, it can execute one and let to the human the possibility to perform the other(s).

Other costs can be considered as human preferences. We can imagine having a list of actions the human likes to perform and another he dislikes. The robot can then allocate the actions following these preferences.

\subsection{Action execution}
\label{subsec:execution}

Once the robot has decided to execute an action ($EXECUTE$ operator), it needs to be able to deal with the \textit{similar} objects introduced before. To do so, we keep the principle that the robot waits until the last moment to take a decision. For example, if the robot and the humans have to pick objects and place them in several similar placements, the robot will first pick an object and only after choose a placement to place it. Then, when the robot has to choose an object, it will choose the one it considers the less costly.

Finally, if the human approaches an object which is involved in the current robot action (e.g. if he places an object in a placement the robot chose), the robot first halts its action. Then, the robot looks if it can find another \textit{similar} object. If it finds one, it continues its action with this object. If not, it waits the human to retreat from the object, and if the human actions did not lead to a new plan it continues its action if possible.

\textbf{Costs used for objects selection:}
Here we choose to put a lower cost on objects accessible only by the robot (we still want to let the maximum choices to the human). Then, we use a simple cost based on distance. In our cost, we get the distances between the agents hands (here we have chosen the right hand) and objects. For objects accessible only by the robot the costs will be proportional to the distance between the robot hand and the objects, leading the robot to choose the closest one. Concerning objects accessible also by the human, the costs will be inversely proportional to the distance between the human hand and the objects, leading the robot to choose the farthest object from the human to minimize the efforts for the human to reach the objects left. 


\section{Results}

\subsection{Task}

To illustrate the work done in this chapter, we use a task adapted for the manipulation abilities of a PR2 robot and inspired from the one in \cite{clodic2014key}. A human and a robot have to build a blocks construction as represented in Fig. \ref{subfig:goal}. At the beginning of the task, the robot and the human have several colored blocks they can access as in Fig. \ref{subfig:setUp}. Two identical placements are set on the table to indicate where to put the two red cubes.

\begin{figure*}[!h]
\centering
	\subfigure[Goal of the task (side view)]{
        \centering
        \includegraphics[width=0.25\textwidth]{figs/Chapter4/BlockGoal.png}
       \label{subfig:goal}
   }
    %~
	\subfigure[One possible initial set-up (top view)]{
        \centering
        \includegraphics[width=0.4\textwidth]{figs/Chapter4/SetUp.png}
       \label{subfig:setUp}
   }
    \caption{Description of the blocks building task. The human and the robot have to build the stack together. We assume that the robot and the human know where are all the available blocks. We would like the robot to adapt as much as possible to the human actions and decisions while avoiding useless or tiresome verbal interactions}
    \label{fig:blocksBuildingTask}
\end{figure*}

\subsection{Illustrative example}

We will first present one possible scenario of the task described earlier which illustrate well the benefits of this work.

\begin{figure*}[!h]
\centering
	\subfigure[Initial plan]{
        \centering
        \includegraphics[width=0.7\textwidth]{figs/Chapter4/init_plan.png}
       \label{subfig:initPlan}
   }
    %~
	\subfigure[The robot chooses to put the red cube in the placement to its right]{
        \centering
        \includegraphics[width=0.4\textwidth]{figs/Chapter4/screen_shot1.jpeg}
       \label{subfig:redCube}
   }
    %~
	\subfigure[The human places his cube in the placement the robot chose]{
        \centering
        \includegraphics[width=0.4\textwidth]{figs/Chapter4/screen_shot2.jpeg}
       \label{subfig:humanPlace}
   }
    %~
	\subfigure[Second computed plan]{
        \centering
        \includegraphics[width=0.7\textwidth]{figs/Chapter4/second_plan.png}
       \label{subfig:secondPlan}
   }
    %~
	\subfigure[The robot adapts by changing its placement choice]{
        \centering
        \includegraphics[width=0.4\textwidth]{figs/Chapter4/screen_shot3.jpeg}
       \label{subfig:robotAdapts}
   }
    %~
	\subfigure[Third computed plan]{
        \centering
        \includegraphics[width=0.4\textwidth]{figs/Chapter4/third_plan.png}
       \label{subfig:thirdPlan}
   }
    \caption{The human and the robot build a blocks construction together. The robot adapts its behavior to the human actions.}
\end{figure*}


The presented scenario starts with the set-up in Fig.~\ref{subfig:setUp}. The plan produced by HATP for this set-up can be found in Fig.~\ref{subfig:initPlan}. As this plan starts with two \textit{analogous} actions for the \textit{X agent} (place a red cube into a placement), the robot selects one and starts to execute it. So, the robot picks the red cube (Fig. \ref{subfig:redCube}) and, at the same time, looks for the consequences of its choice in the plan. As both agents own only one red cube, in the new plan computed by the robot (Fig.~\ref{subfig:secondPlan}) the human needs to place the second red cube. After picking its red cube the robot starts to place it on the placement to its right. However, the human picks his red cube and places it in the very same placement (Fig.~\ref{subfig:humanPlace}). So, the robot stops its movement and adapts by placing its cube in the other placement (Fig.~\ref{subfig:robotAdapts}). Then, the human places the stick on the red cubes. In this scenario, we have chosen to set the robot into the \textbf{negotiation} mode. As the next action is allocated to the \textit{X agent}, the robot asks the human if he wants to do it (\textit{"Do you want to place the blue cube?"}). The human answers yes, leading the robot to compute the new plan in Fig.~\ref{subfig:thirdPlan} where the human have to place the first blue cube and the robot the second one. Finally, the human and the robot perform their last actions and achieve the goal.


\subsection{Quantitative results}

In order to evaluate our system, we run it in simulation using the blocks building scenario. Different set-ups was used as initial state of the task: we randomized the number of cubes of each color in the environment and their position (accessible by the robot or the human). The robot was confronted to a simulated human with different types of behaviors:

\begin{itemize}
\item \textbf{The "kind" human:} this human performs all actions that are feasible only by him and, when there is an action feasible by both him and the robot, he either chooses to perform it with 50 \% chance (noted "50\%-K"), systematically chooses to perform it (noted "hurry-K") or systematically chooses not to perform it (noted "lazy-K"). The "kind" human answers to robot questions and adapts his behavior to what the robot verbalizes (he does an action if the robot asks him and stops his action if the robot says it will perform it).
\item \textbf{The "stubborn" human:} contrary to the "kind" human, this human does not react nor comply to robot verbalization: he will not change his decision to perform or not an action (noted "50\%-S", "hurry-S" and "lazy-S").
\end{itemize}

We compare the two different modes of our system ("negotiation" and "adaptation", see Sec.~\ref{subsec:allocation}) to a reference system (RS) where the Shared Plans are completely refined during plan elaboration (no \textit{X agent} nor \textit{similar} object). Concerning what the robot says, we choose to run the reference system with three different modes:
\begin{itemize}
\item the robot verbalizes nothing (noted "RS-none").
\item the robot informs the human when he has to perform an action (noted "RS-human").
\item the robot informs the human when he has to perform an action and when it will act (noted "RS-all").
\end{itemize}

We measured the number of verbal interactions between the human and the robot (Tab.~\ref{tab:incompatible}) and the number of human/robot incompatible decisions (Tab.~\ref{tab:incompatible}): either both decide to perform the same action (and the robot stops its own action to avoid the conflict) or both decide not to perform the action (the robot first asks the human to perform the action after a predefined time and, if after another period the human has still not executed the action, the robot looks for a new plan where it can proceed).

\begin{table*}[!h]
  \begin{tabular}{|c||c|c|c|c|c||}
  \hline
     & \textbf{RS-none} & \textbf{RS-human} & \textbf{RS-all} & \textbf{Neg} & \textbf{Adapt} \\
  \hline
  \hline
     \textbf{50\%-K} & 0.6 & 0.1 & 0.0 & 0.0 & 0.0 \\
     \textbf{(SD)} & (0.52) & (0.32) & (0.0) & (0.0) & (0.0) \\
  \hline
     \textbf{hurry-K} & 0.3 & 0.3 & 0.0 & 0.0 & 0.0 \\
     \textbf{(SD)} & (0.48) & (0.48) & (0.0) & (0.0) & (0.0) \\
  \hline
     \textbf{lazy-K} & 0.9) & 0.0 & 0.0 & 0.0 & 0.0 \\
     \textbf{(SD)} & (0.32) & (0.0) & (0.0) & (0.0) & (0.0) \\
  \hline
     \textbf{50\%-S} & 0.5 & 0.7 & 0.6 & 0.0 & 0.0 \\
     \textbf{(SD)} & (0.53) & (0.67) & (0.52) & (0.0) & (0.0) \\
  \hline
     \textbf{hurry-S} & 0.3 & 0.3 & 0.3 & 0.0 & 0.0 \\
     \textbf{(SD)} & (0.48) & (0.48) & (0.48) & (0.0) & (0.0) \\
  \hline
     \textbf{lazy-S} & 0.9 & 0.9 & 0.9 & 0.0 & 0.0 \\
     \textbf{(SD)} & (0.32) & (0.32) & (0.32) & (0.0) & (0.0) \\
  \hline
  \end{tabular}
   \caption{Results for the reference system (RS) and the proposed system (Neg for the negotiation mode and Adapt for the adaptation mode). Number of incompatible decisions between the human and the robot (i.e. either both agents decide to perform the same action or both decide not to perform a given action). The numbers correspond to means in 10 runs and their associated standard deviations.}
   \label{tab:incompatible} 
\end{table*}

We also measured execution time but no significant difference was found between the different conditions. Indeed, this criterion is not pertinent here since, as all actions concern the same stack, they need to be performed one after the other. Consequently, there is no significant difference time between the different options.

\begin{table*}[!h]
  \begin{tabular}{|c||c|c|c|c|c||}
  \hline
     & \textbf{RS-none} & \textbf{RS-human} & \textbf{RS-all} & \textbf{Neg} & \textbf{Adapt} \\
  \hline
  \hline
     \textbf{50\%-K} & 0.4 & 2.1 & 5.1 & 1.2 & 0.0 \\
     \textbf{(SD)} & (0.52) & (0.32) & (0.32) & (0.63) & (0.0) \\
  \hline
     \textbf{hurry-K} & 0.0 & 2.1 & 5.1 & 1.2 & 0.0 \\
     \textbf{(SD)} & (0.0) & (0.32) & (0.32) & (0.63) & (0.0) \\
  \hline
     \textbf{lazy-K} & 0.9 & 2.1 & 5.1 & 1.2 & 0.0 \\
     \textbf{(SD)} & (0.32) & (0.32) & (0.32) & (0.63) & (0.0) \\
  \hline
     \textbf{50\%-S} & 0.4 & 2.1 & 5.1 & 1.2 & 0.0 \\
     \textbf{(SD)} & (0.52) & (0.32) & (0.32) & (0.63) & (0.0) \\
  \hline
     \textbf{hurry-S} & 0.0 & 2.1 & 5.1 & 1.2 & 0.0) \\
     \textbf{(SD)} & (0.0) & (0.32) & (0.32) & (0.63) & (0.0) \\
  \hline
     \textbf{lazy-S} & 0.9 & 2.1 & 5.1 & 1.2 & 0.0 \\
     \textbf{(SD)} & (0.32) & (0.32) & (0.32) & (0.63) & (0.0) \\
  \hline
  \end{tabular}
   \caption{Results for the reference system (RS) and the proposed system (Neg for the negotiation mode and Adapt for the adaptation mode). Number of verbal interactions (i.e. question asked by the robot in the negotiation mode or an information given with the reference system). The numbers correspond to means in 10 runs and their associated standard deviations.}
   \label{tab:verb}
\end{table*}

We can see that with the reference system, even when the human is "kind", the less the robot speaks, the more there are incompatible decisions between the human and the robot. Indeed, with this system, as the robot decides in advance the actions allocation, it needs to inform the human in order for him to adapt to the robot plan. It is worst when the human is "stubborn" since, even if the robot informs the human, the conflicts remain. Note that even in the case where the robot was not supposed to verbalize anything (RS-none), there is still verbalization: this is due to the incompatible decisions where the human and the robot both choose not to execute the action, the robot tries to solve the conflict by asking the human to execute the action.

With our new system, we can see that the robot is able to avoid conflicts in all cases without being too talkative (or without being talkative at all for the adaptation mode). This system allows more freedom to the human by letting him the choice to perform or not actions: with the "stubborn" human who performs the actions he wants to perform the robot still avoids conflicts without having to be more "chatty" (this is not the case with the reference system).

Finally, here the adaptation mode performs better than the negotiation one since the human is simulated and always performs his actions in time. However, in a real context, the negotiation mode could have the benefit to ensure the absence of conflicts even if the robot is a little more talkative. Moreover, the human can be more comfortable with a robot which directly asks when (and only when) there is a decision to take compared to a robot which has unnecessary waiting time. Such measure of "satisfaction" will be treat in Chapter~\ref{ch:Eval} where a study with real participants has been made.

\section{Conclusion}

In this chapter we shown how we enable the robot to compute and execute more flexible Shared Plans. In these new plans, the needed decisions on who will execute an action and with which objects are let to the execution. A number of the presented algorithms involve cost estimation in order to decide between options. In the current system, simple costs are used but could be easily replaced by more elaborate ones. For instance, a finer estimation of action costs based on geometric reasoning and human efforts would allow better informed choice for action or object selection. Another interesting issue would be to integrate the estimation of accumulated costs of all actions remaining in the plan.

The benefits of this work have been demonstrated with an illustrative example and simulation results. We will show in Chapter~\ref{ch:Eval}) more complete simulation results which also include the work of the previous chapter as well as results with the system running in a real situation.


\ifdefined\included
\else
\bibliographystyle{StyleThese}
\bibliography{These}
\end{document}
\fi
